<!DOCTYPE html>
<html lang="en" style = "scroll-behavior: smooth;">
	<head>
		<script type="text/javascript"
		  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale = 1.0">
		<title>Neural Networks from Scratch</title>
		<link rel="icon" href="images/jm.png">
		<link rel = "stylesheet" href="style.css">
	</head>
	<nav class = "navbar">
		<div class = "max-width">
			<div class = "logo"><a href="#">Neural Networks from Scratch</a></div>
			<ul class = "menu">
				<li><a href="https://joseph-ma.com">Go Back</a></li>
			</ul>
		</div>
	</nav>
	<body>
		<div class = "top_info">
			<div class = "pic1">
				<img src = "images/nn.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "information">
				<div class = "text">About</div>
				<p>
					Neural Networks and deep learning were always a black box to me. You put in some data and it gives you back a prediction... magic! Even when using <b><a href = "https://en.wikipedia.org/wiki/TensorFlow" target = "_blank">TensorFlow</a></b> or <b><a href = "https://en.wikipedia.org/wiki/PyTorch" target = "_blank">PyTorch</a></b> it wasn't crystal clear what was going on so I decided to create one from scratch. No TensorFlow, no PyTorch just NumPy, Python and some Calculus with Linear Algebra. Neural Networks are a type of supervised learning algorithm where we have a lot of data usually labeled (x<sub>i</sub>, y<sub>i</sub>). If we train the model enough, lets say from (x<sub>1</sub>, y<sub>1</sub>) to (x<sub>10000</sub>, y<sub>10000</sub>). The model should learn and if we give it a value x<sub>10001</sub> (which it has never seen before), it should be able to accurately tell you the output is y<sub>10001</sub>.
					
					<br><br>
					I chose a couple of data sets to train my neural network on. You may find the google colab pages below:
					<ul class = "shift">
						<li><b><a href = "https://colab.research.google.com/drive/1aCvAY5AyZ4AWchLSX1UnwLq4riOsK1Cj?usp=sharing" target = "_blank">The IRIS flower data set</a></b>.</li>
						<li><b><a href = "https://colab.research.google.com/drive/1NTBunhwXD_-jbkvhWAewRKjYwigOrHNs?usp=sharing" target = "_blank">The MNIST data set</a></b>.</li>
<!-- 						<li>CIFAR-10 dataset</li>
						<li>Fashion MNIST Dataset</li> -->
					</ul>
					<br>
					To work through the steps, I will be using the Iris Flower Dataset as an example but I wrote the code such that we have a Layer and Neural Network class so that we may conveniently adjust the number of layers and neurons depending on the nature of our dataset. <br>
				</p>
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "filters_about">
				<div class = "text">Structure of the Neural Network with the Iris Dataset</div>
				<p>
					The Iris Dataset consists of 150 samples of data. Each sample of data has 4 features with a label corresponding to the flower those features belong to.<br><br>
					<b>Features</b> include:
					<ul class = "shift">
						<li>Sepal Length (Denoted as Input x1 in the network) </li>
						<li>Sepal Width (Denoted as Input x2 in the network)</li>
						<li>Petal Length (Denoted as Input x3 in the network)</li>
						<li>Petal Width (Denoted as Input x4 in the network)</li>
					</ul><br>
					<b>Species</b> include:
					<ul class = "shift">
						<li>Iris Setosa (Denoted as Output \( \hat{y} \)1) </li>
						<li>Iris Versicolor (Denoted as Output \( \hat{y} \)2)</li>
						<li>Iris Virginica (Denoted as Output \( \hat{y} \)3)</li>
					</ul><br>

					Our goal is for the neural network to be able to predict the species of an unseen dataset. For example if we have an <b>input [7.7, 2.6, 6.9, 2.3]</b>. Where <b>sepal length = 7.7 cm</b>, <b>sepal width = 2.6 cm</b>, <b>petal length = 6.9 cm</b> and <b>petal width = 2.3 cm</b> and the neural network has never seen this before, it should be able to accurately predict which species this is. The neural network would output something like this <b>[0.013, 0.022, 0.96]</b>. Where <b><strong>\( \boldsymbol{\hat{y}} \)</strong>1 = 0.013</b> meaning there is a <b>1.3% chance it is a Iris Setosa</b>, <b><strong>\( \boldsymbol{\hat{y}} \)</strong>2 = 0.022</b> meaning there is a <b>2.2% chance it is a Iris Versicolor</b> and <b><strong>\( \boldsymbol{\hat{y}} \)</strong>3 = 0.96</b> meaning there is a <b>96% chance it is a Iris Virginica</b>. This will be compared to the true output which should be <b>[0, 0, 1]</b> assuming the species is really an Iris Virginica.

				</p>
			</div>
			<div class = "pic2">
				<img src = "images/structure.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "filters_about">
				<div class = "text">Training Steps</div>
				<p>
					We can summarize the training in about 5 steps here though I will go into more details later.
					<ol class = "shift"><br>
						<b><li>Preparing Data</li></b>
						The original data contains 150 data sets with X-features corresponding to y-labels. We split 80% of the x-features with corresponding y-labels for training. This <b>X-train, y-train</b> data will be used for <b>step 2 to 4</b>. The other 20% of the data will be reserved for <b>step 5</b>. In order to evaluate the neural network's abilities, it will not be allowed to see this data set. This way we will know whether it truly learned the model.
						<b><li>Forward Propagation</li></b>
						Pass the X-train data through the network and get the <strong>\( \boldsymbol{\hat{y}} \)</strong> predicted results. 
						<b><li>Evaluate Loss</li></b>
						Figure out how far our predicted values deviated from the true values.
						<b><li>Backpropagation, Gradient Descent and adjusting weight/biases</li></b>
						This is where we tell the network to learn from its mistakes. The main idea behind this is figuring out how which direction to tune each weight and bias in the network to minimize the loss. <b> --> We repeat step 2 to 4 several times until the network has learned the model</b> .
						<b><li>Evaluate the model</li></b>
						Now we test our network on unseen data. Tell it to figure out what the Y-test labels are to the X-test data. We mark the accuracy/score at the end based on how many of those data points the network got right.
					</ol><br>
				</p>
			</div>
			<div class = "pic2">
				<img src = "images/steps.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "filters_about_2">
				<div class = "text">Weights, biases and notation</div>
				<p>
					To simplify, i'm just going to look at the first and last neuron of layer 0 (I'll consider the input layer as layer 0) and the first and last neuron of layer 1 (The first hidden layer). <br>
					<b>Weights W<sub>i(j,k)</sub></b>
					<li>Each neuron's has a set of unique weights associated with it which is applied to the input from the previous layer. We can denote each unique weight as <b>W<sub>i(j,k)</sub></b> where <b>i - layer, j - neuron from, k - neuron to</b>. </li>
					<b>Biases b<sub>ik</sub></b>
					<li>Each neuron except the ones from the input (layer 0) have a unique bias. We can denote each unique bias as <b>b<sub>ik</sub></b> where <b>i - layer, k - neuron to</b>. </li>
					<b>Weighted inputs Z<sub>ik</sub></b>
					<li>The value entering each neuron is a sum of the weighted inputs + its bias. For example, the value entering the first neuron in layer 1 is Z<sub>ij</sub>. Its value can be calculated as a weighted sum of the inputs plus its bias: <b>Z<sub>11</sub> = W<sub>1(1,1)</sub>*x<sub>1</sub> + W<sub>1(2,1)</sub>*x<sub>2</sub> + W<sub>1(3,1)</sub>*x<sub>3</sub> + W<sub>1(4,1)</sub>*x<sub>4</sub> + b<sub>11</sub></b></li>
					<b>Outputs a<sub>ik</sub></b>
					<li>The inputs are then run through an activation function which will be covered below. In this neural network we will be using two activation functions <b>ReLU</b> and <b>SoftMax</b>. The output of the function is the output value of the neuron which will serve as the values for the next layer assuming we are not at the final layer.</li>
				</p>
			</div>
			<div class = "pic4">
				<img src = "images/weights.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/forward1.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/forward2.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Activation Function: ReLU</div>
				<p>
					The activation function introduces non-linearity into our neural network. Without the use of activation functions, our neural network will just be a bunch of linear combinations - that's going to be useless. If a connection is not strong enough, it will just not "fire" the neuron. What ReLU does is it only passes the value z<sub>ik</sub> on if the value exceeds 0. If it is below 0, the connection is simply not strong enough and the output a<sub>ik</sub> will be 0. <br>
					<br>
					In simple terms:<br>
					If z<sub>ik</sub> > 0 then a<sub>ik</sub> = z<sub>ik</sub><br>
					Otherwise:
					a<sub>ik</sub> = 0
					<br><br>
					We can also write the ReLU function as <b>f(x) = max(0, x)</b>.

				</p>
			</div>
			<div class = "pic8">
				<img src = "images/relu.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br><br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/forward3.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/forward4.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/forward5.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/forward6.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Activation Function: SoftMax</div>
				<p>
				Softmax is another activation function that allows us to take the output as percentages. At the moment, the logits <b>z<sub>31</sub>, z<sub>32</sub>, z<sub>33</sub> </b> on their own do not represent percentages. The softmax function addresses this by exponentiating each logit, summing these exponentials, and then dividing each exponentiated logit by this total sum. This process effectively converts the logits into a set of values that are interpretable as the model's confidence in each possible outcome, with all the values summing up to 100%. In other words, softmax allows us to evaluate the confidence level of the machine learning model in its predictions, presenting the output as percentages.

				</p>
				<p>$$\text{softmax}(z_{ik}) = \frac{e^{z_{ik}}}{\sum_{c=1}^{N} e^{z_{ic}}}$$</p>
				<p>
				<ul class = "shift">
					<li><b>Z</b> - the logit</li>
					<li><b>i</b> - the network layer (usually the last layer if we are using SoftMax)</li>
					<li><b>c, k</b> - the neuron number</li>
					<li><b>N</b> - the number of logits</li>
				</ul>
				</p>
				
			</div>
			<div class = "pic8">
				<img src = "images/softmax.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br><br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Evaluate Loss: Categorical Cross-Entropy Loss</div>
				<p>
				So how do we evaluate how close the model is to the true output? We use a loss function called categorical cross-entropy loss. The formula calculates the loss by summing the negative logarithms of the predicted probabilities (<strong>\( \boldsymbol{\hat{y}} \)<sub>c</sub></strong>) for each category, weighted by the actual labels (<strong>\( \boldsymbol{\hat{y}} \)<sub>c</sub></strong>) across all M categories. 

				</p>
				<p>$$L = -\sum_{c=1}^{M} y_c \log(\hat{y}_c)$$</p>
				<p>
					<ul class = "shift">
					<li><b>c</b> - the output neuron number/ category we are predicting</li>
					<li><b>M</b> - the number of output neurons/ number of categories we are predicting</li>
					<li><b><strong>\( \boldsymbol{\hat{y}} \)</strong><sub>c</sub></b> - the c<sub>th</sub> predicted value </li>
					<li><strong>\( \boldsymbol{y_c} \)</strong> - the true value (from y train)</li>
				</ul>
				<br>
				<p>In the examples we can see what happens when our model is <b>96%</b> sure it is Iris Virginica and it truly is Iris Virginica: the loss is only <b>0.041</b>. When the model is <b>68%</b> sure it is Iris Versicolor, but it is actually Iris Virginica: the loss increases to <b>1.386</b>. 
				</p>
			</div>
			<div class = "pic9">
				<img src = "images/loss.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Minimizing loss: Gradient Descent</div>
				<p>
				So how do we get our model to do better? We can aim to minimize the loss. To find the minimum of a function we can solve for the derivative. It's true that when the derivative of a function is at zero we are at a minimum or maximum but sometimes it is highly impractical or even impossible to solve for that. To approach this from another way, we apply gradient descent. Gradient descent finds the current instantaneous slope and updates our parameter based on that. <br><br>
				Let's say we want to see how much we need to shift our current weight <b>w</b> by in order to reduce the value outputed by the <b>Loss</b> function which we can put for example as <b>Loss = (w + 5)<sup>2</sup></b>. If we differentiate the <b>Loss</b> function with respect to <b>w</b> we can solve for the <b>instantaneous slope</b> of the <b>Loss function</b> at any point along the x-axis. We update our <b>w</b> parameter and move closer to the optimal value to minimize the <b>Loss</b> through subtracting a learning rate <b>α - alpha</b> (0.3 here) * the <b>instantaneous slope at that point</b>. <br><br>
				Initial <b>w = -7, L = 4</b><br>
				After 1st iteration: <b>w = -5.8, L = 0.64</b><br>
				After 2nd iteration: <b>w = -5.32, L = 0.1024</b><br>
				After 3rd iteration: <b>w = -5.128, L = 0.016384</b><br>
				After 4th iteration: <b>w = -5.0512, L = 0.00262144</b><br>
				After 5th iteration: <b>w = -5.02048, L = 0.0004194304</b><br>

				</p>
			</div>
			<div class = "pic9">
				<img src = "images/gradients.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Global minimum of the loss function is not gauranteed</div>
				<p>
				Here we have an example of where gradient descent may not lead us to the global minimum of the loss function. The loss function is given here by <b>L = w<sup>4</sup> + w<sup>3</sup> - 3w<sup>2</sup> + 5.5</b>. If we differentiate the loss function with respect to that particular weight, we get <b>4w<sup>3</sup> + 3w<sup>2</sup> - 6w</b>. If our initial weight is initialized to <b>w = 1</b>, we will \inch towards the local minimum of the Loss function found at <b>(w = 0.91, L = 4.46)</b>, while there is a more desired optimal global minimum found at <b>(w = -1.66, L = 0.25)</b>. Since we have no idea where the global minimum might be, this problem will be difficult to solve. Where we initialize our weight largely determines which local/global minimum we will fall upon. One method to combat this is to train the neural network multiple instances with different randomized weight initializations and find the one that provides us the best minimum loss. The learning rate <b>α - alpha</b> is 0.1 here. A high learning rate <b>α - alpha</b> ensures us we travel across the axis faster but the disadvantage is that we may overshoot the targeted minimum. On the other hand, a low learning rate ensures we will not overshoot, but the disadvantage is that we may take too long to converge to the minimum.
				<br><br>
				
				Initial <b>w = 1, L = 4.5</b><br>
				After 1st iteration: <b>w = 0.9, L = 4.4551</b><br>
				After 2nd iteration: <b>w = 0.9054, L = 4.454942</b><br>
				After 3rd iteration: <b>w = 0.905835, L = 4.454941</b><br>
				After 4th iteration: <b>w = 0.905866, L = 4.454941</b><br>
				After 5th iteration: <b>w = 0.905869, L = 4.454941</b><br>
				</p>
			</div>
			<div class = "pic9">
				<img src = "images/grad2.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Initializing Weights and Biases</div>
				<script type="text/x-mathjax-config">
				  MathJax.Hub.Config({
				    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
				  });
				</script>
				<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML"></script>
				<p>
				As shown previously, we have no idea where the optimal weights or biases may lie such that our neural network will provide the optimal performance with minimal loss. Therefore, the weights are initialized randomly using a method called <b>He initialization - discovered by Kaiming He</b> and the biases are initialized to zero. <br><br>
				The Kaiming He initialization works by picking values for each weight from a normal distribution with the standard deviation scaled by <strong>$\sqrt{\frac{2}{\text{fan-in}}}$</strong>, <b>fan-in</b> here means the dimensions of the input layer. So if we were to initialize weights for <b>Layer 1</b>, <b>fan-in would be 4</b>. If we were to initialize weights for <b>Layer 2</b>, <b>fan-in would be 5</b> and so on. In a normal distribution with the standard deviation at 1 and the mean at 0, we would have about a 68% chance of getting a value between (-1, 1), 95% chance of getting a value between (-2, 2) and 99.7% chance of getting a value between (-3, 3). If we scale by <strong>$\sqrt{\frac{2}{\text{fan-in}}}$</strong>, it would be 68% chance between (-<strong>$\sqrt{\frac{2}{\text{fan-in}}}$</strong>, <strong>$\sqrt{\frac{2}{\text{fan-in}}}$</strong>), 95% chance between (<strong>-2$\sqrt{\frac{2}{\text{fan-in}}}$</strong>, <strong>2$\sqrt{\frac{2}{\text{fan-in}}}$</strong>) and 99.7% chance between (<strong>-3$\sqrt{\frac{2}{\text{fan-in}}}$</strong>, <strong>3$\sqrt{\frac{2}{\text{fan-in}}}$</strong>). In general, we can integrate between 2 values <b>a</b> and <b>b</b> in the normal distribution function to find the probability of retrieving a value between a and b. Keep in mind that the probability of getting an exact value is 0 because there are infinitely possible many values (eg. there are infinite values 0.1, 0.11, 0.111... between 0.1 and 0.12).
				</p>
			</div>
			<div class = "pic9">
				<img src = "images/he.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Backpropagation: Time to start fixing our weights/biases</div>
				<p>From what we saw above about <b>Minimizing loss: Gradient Descent</b>, we could begin to do the same thing for each and every <b>weight and bias</b> we have in our neural network. If we scan through the network, we would be able to see that we have <b>73 parameters in total</b> to update and fine tune. <b>60 weights and 13 biases total</b>. Remember what we were doing in <b>Minimizing loss: Gradient Descent</b>? Find the derivative of the Loss function with respect to the w variable to discover <b>how much w affects the Loss function</b> then we can <b>change the value of w to minimize our Loss</b>. <br><br>
				We are going to do the exact same thing through our network. This time it will be a little tricker though. No longer are we only differentiating with respect to one variable but we are doing it with respect to many. This is why we need partial differentials. We will calculate the partial derivative of the Loss function with respect to every single individual one of these <b>60 weights and 13 biases</b> such that they may each find their own optimal direction to shift towards. <br><br>
				Time to implement the <b>chain rule</b>. We always start off from the end of the network and work our way to the beginning because the Loss function output is closest to the most recent weights and biases of layer 3. To find how much each weight and bias from layer 3 affects the loss function, we will have to retrace our steps and see what is the most recent thing that affects the loss function (that would be the output \( \hat{y} \)<sub>k</sub>)). What's the most recent thing that affects \( \hat{y} \)<sub>k</sub>? That would be Z<sub>3k</sub> also known as the logits. What's the most recent thing that affects Z<sub>3k</sub>? That would be W<sub>3jk</sub> and b<sub>3k</sub>. Great! This is what we are interested in updating. 

				</p>
			</div>
			<div class = "pic9">
				<img src = "images/back.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward2.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward3.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward4.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward5.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward6.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward7.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward8.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward9.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward10.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward11.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward12.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward13.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward14.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward15.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward16.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward17.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward18.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/backward19.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/backward20.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br><br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Going from Math -> Code</div>
				<p>Here we analyze how we might write the code for the backpropagation algorithm. In the example, we our trying to solve \( \frac{\partial L}{\partial w_{1jk}} \) and \( \frac{\partial L}{\partial b_{k}} \). Notice another introduction of two variables k and k'. The purpose of this is because each <b>a<sub>i</sub></b> affects every <b>z<sub>(i + 1)</sub></b>. For example <b>a<sub>11</sub></b> will be found in <b>z<sub>21</sub>, z<sub>22</sub> ... z<sub>25</sub></b>. Therefore we have to introduce another variable <b>k'</b> where <b>k'</b> is not necessarily equal to <b>k</b>. Since the calculation for \( \frac{\partial L}{\partial w_{1jk}} \) and \( \frac{\partial L}{\partial b_{k}} \) are almost the same, I will just focus on \( \frac{\partial L}{\partial w_{1jk}} \). <br><br>
					$$\frac{\partial L}{\partial w_{1jk}} = \frac{\partial L}{\partial z_{2k'}} \cdot \frac{\partial z_{2k'}}{\partial a_{1k}} \cdot \frac{\partial a_{1k}}{\partial z_{1k}} \cdot \frac{\partial z_{1k}}{\partial w_{1jk}}$$
					The term \( \frac{\partial L}{\partial z_{2k'}} \) was already calculated in layer 2. The term \( \frac{\partial z_{2k'}}{\partial a_{1k}} \) is simply the weights associated with layer 2. This is the value returned from the previous function call so we can pass it in as parameter \( \frac{\partial L}{\partial a_{1k}} \) with the variable named <b>dL_da</b>. All that is left to do is to calculate \( \frac{\partial a_{1k}}{\partial z_{1k}} \) which is the <b>derivative of ReLU</b>. <i><b>(</b>*Note that if we are coming from the output layer, we will omit the multiplication with <b>ReLU'</b>. I wrote it such that the initial incoming <b>dL_da</b> will automatically be the calculated \( \frac{\partial L}{\partial z_{3k}} \)<b>)</b></i>. And also calculate \( \frac{\partial z_{1k}}{\partial w_{1jk}} \) which is <b>a<sub>0jk</sub></b> or in this case, the initial inputs <b>(x<sub>1</sub>,x<sub>2</sub>,x<sub>3</sub>,x<sub>4</sub>)</b>. For this example, we may not have any more weights or biases to update but in the general case where we have to keep going backwards, we can simply return <b>prev_dL_da</b> = \( \frac{\partial L}{\partial a_{(i-1)k}} \) so that the next time we call the <b>backward function</b>, we will have our <b>dL_da</b> ready to go. 

				</p>
			</div>
			<div class = "pic9">
				<img src = "images/code.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br><br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">The general equation</div>
				<p>
					In general we can come up with the following formulas: <br><br>
					$$\frac{\partial L}{\partial w_{ijk}} = \frac{\partial L}{\partial a_{ik}} \cdot \frac{\partial a_{ik}}{\partial z_{ik}} \cdot \frac{\partial z_{ik}}{\partial w_{ijk}}$$
					$$\frac{\partial L}{\partial b_{ik}} = \frac{\partial L}{\partial a_{ik}} \cdot \frac{\partial a_{ik}}{\partial z_{ik}} \cdot \frac{\partial z_{ik}}{\partial b_{ik}}$$<br>
					<ul class = "shift">
						<li>
							\( \frac{\partial L}{\partial w_{ijk}} \) : Derivative of <b>Loss function</b> with respect to the <b>weight</b> in the <b>i<sup>th</sup> layer</b>, corresponding to the value coming from the <b>j<sup>th</sup> neuron</b> in <b>layer<sub>i-1</sub></b>, and the value going in the <b>k<sup>th</sup> neuron</b> in <b>layer<sub>i</sub></b>. 
						</li>
						<br>
						<li>
							\( \frac{\partial L}{\partial b_{ik}} \) : Derivative of <b>Loss function</b> with respect to the <b>bias</b> in the <b>i<sup>th</sup> layer</b>, corresponding to the value going in the <b>k<sup>th</sup> neuron</b> in <b>layer<sub>i</sub></b>. 
						</li>
					</ul>

				</p>
			</div>
			<div class = "potentiometer2">
				<div class = "text">Evaluate Model</div>
				<p>Once we have shifted the weights and biases for a certain number of defined <b>epochs</b> (how many times we pass each and every single data point in (X-train, y-train) through the model, we should end up with some pretty good looking <b>weights</b> and <b>biases</b>. <br><br>

				The data from <b>X-test</b> would be sent through the model and for every <b>X-test</b>, there will be an output result <strong>\( \hat{y} \)</strong>. We will then compare <strong>\( \hat{y} \)</strong> with the true answers found in <b>y-test</b>. The total correct / total number of tests would be the accuracy. <br><br>
				In most categorical models, the output will come out something like <b>[0.013, 0.022, 0.96]</b>. To evaluate this, we simply select the largest value in this list since that is the category that the model most confidently thinks the answer is. We evaluate it with the true answer. If the true answer is <b>[0, 0, 1]</b>, it will be counted as correct. If it is <b>[0, 1, 0]</b> or <b>[1, 0, 0]</b> that would be incorrect. <br><br>
				Next I am going to run through an example of what it looks like when we try to make a prediction with some pretty good weights and biases. 
				</p>
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Walking through a Trained Model</div>
				<p>Lets say our model is trained and ready to go!<br><br>
					Our weights and biases are pretty well optimized which are shown in the picture.<br><br>
					Lets do a mini evaluation with one data point (usually we evaluate on many more data points to see how well it really performs but this is an example). Below is a data pair from (X-test and y-test).<br><br>
					<ul class = "shift">
						<li><b>x1 - Sepal Length: 7.7 cm</b></li>
						<li><b>x2 - Sepal Width: 2.6 cm</b></li>
						<li><b>x3 - Petal Length: 6.9 cm</b></li>
						<li><b>x4 - Petal Width: 2.3 cm</b></li>
					</ul>
					<br><br>
					The true answer is <b>Iris Virginica: [0, 0, 1]</b>, but shush we're not going to tell the model that. Let's see if it can figure it out on its own <b>:)</b>
				</p>
			</div>
			<div class = "pic9">
				<img src = "images/my_weights.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/test1.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/test2.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "filters">
			<div class = "pic6">
				<img src = "images/test3.png" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pic7">
				<img src = "images/test4.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<!-- <div class = "filters">
			<div class = "potentiometer">
				<div class = "text">Exploding and Vanishing Gradients</div>
				<p>
				Here we have an example of where gradient descent may not lead us to the global minimum of the loss function. The loss function is given here by <b>L = w<sup>4</sup> + w<sup>3</sup> - 3w<sup>2</sup> + 5.5</b>. If we differentiate the loss function with respect to that particular weight, we get <b>4w<sup>3</sup> + 3w<sup>2</sup> - 6w</b>. If our initial weight is initialized to <b>w = 1</b>, we will \inch towards the local minimum of the Loss function found at <b>(w = 0.91, L = 4.46)</b>, while there is a more desired optimal global minimum found at <b>(w = -1.66, L = 0.25)</b>. Since we have no idea where the global minimum might be, this problem will be difficult to solve. Where we initialize our weight largely determines which local/global minimum we will fall upon. One method to combat this is to train the neural network multiple instances with different randomized weight initializations and find the one that provides us the best minimum loss. The learning rate <b>α - alpha</b> is 0.1 here. A high learning rate <b>α - alpha</b> ensures us we travel across the axis faster but the disadvantage is that we may overshoot the targeted minimum. On the other hand, a low learning rate ensures we will not overshoot, but the disadvantage is that we may take too long to converge to the minimum.
				<br><br>
				
				Initial <b>w = 1, L = 4.5</b><br>
				After 1st iteration: <b>w = 0.9, L = 4.4551</b><br>
				After 2nd iteration: <b>w = 0.9054, L = 4.454942</b><br>
				After 3rd iteration: <b>w = 0.905835, L = 4.454941</b><br>
				After 4th iteration: <b>w = 0.905866, L = 4.454941</b><br>
				After 5th iteration: <b>w = 0.905869, L = 4.454941</b><br>
				</p>
			</div>
			<div class = "pic9">
				<img src = "images/grad2.png" class = "pic" width = "100%" alt = "">
			</div>
		</div> -->

	</body>
</html>